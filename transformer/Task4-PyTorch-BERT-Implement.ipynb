{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "realistic-armor",
   "metadata": {},
   "source": [
    "**遗留问题**\n",
    "* segment的padding怎么ignore\n",
    "* wordpiece 怎么用\n",
    "* torch.gather怎么用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hourly-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from random import random, randrange, randint, shuffle\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import EncoderLayer\n",
    "from utils import pad_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-purpose",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
    "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word2idx[w] = i + 4\n",
    "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence to idx\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "judicial-brand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 15, 25, 14, 22, 21, 37],\n",
       " [32, 37, 38, 31, 27, 24, 4, 19, 12, 14],\n",
       " [4, 12, 14, 18, 15, 25, 14, 16],\n",
       " [11, 38, 10, 17, 28, 5, 29],\n",
       " [6, 13, 24],\n",
       " [20, 14, 37],\n",
       " [7, 25, 14, 34, 16],\n",
       " [22, 21, 34, 35, 30, 33, 14],\n",
       " [22, 21, 34, 19, 36, 38, 39, 8, 27, 23, 26, 9]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-dialogue",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "根据概率随机mask或者替换一句话中15%的token，还需要拼接任意两句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vulnerable-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_len = 30\n",
    "max_pred = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "personal-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "def zero_padding(input_ids, segment_ids, max_len, max_pred, n_pred, masked_pos, masked_tokens):\n",
    "    n_pad = max_len - len(input_ids)\n",
    "    input_ids.extend([0] * n_pad)\n",
    "    segment_ids.extend([0] * n_pad)\n",
    "    \n",
    "    # zero padding tokens\n",
    "    if max_pred > n_pred:\n",
    "        n_pad = max_pred - n_pred\n",
    "        masked_tokens.extend([0] * n_pad)\n",
    "        masked_pos.extend([0] * n_pad)\n",
    "    return input_ids, segment_ids, masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "going-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机MASK\n",
    "def mask_lm(input_ids, max_pred):\n",
    "    # 单句要预测的token个数(15%)\n",
    "    n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
    "    # 候选mask id 列表， 特殊标记不可mask\n",
    "    cand_maked_pos = [\n",
    "        i for i, token in enumerate(input_ids)\n",
    "        if token != word2idx['[CLS]'] and token != word2idx['[SEP]']\n",
    "    ]\n",
    "    shuffle(cand_maked_pos)\n",
    "    masked_tokens, masked_pos = [], []\n",
    "    for pos in cand_maked_pos[:n_pred]:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random() < 0.8:  # 80% 替换成[MASK]\n",
    "            input_ids[pos] = word2idx['[MASK]']\n",
    "        elif random() > 0.9:  # 10% 替换成任意词\n",
    "            index = randint(0, vocab_size - 1)\n",
    "            while index < 4:  # 特殊标记不可替换\n",
    "                index = randint(0, vocab_size - 1)\n",
    "            input_ids[pos] = index\n",
    "    return masked_pos, masked_tokens, n_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complimentary-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取positive和negative样本, 比例为1:1\n",
    "def batch_sampler(batch_size, token_list, max_pred, max_len):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # randrange(stop):返回一个随机数\n",
    "        # 如果tokens_a_index + 1 = tokens_b_index, 则为positive，否则 negative\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        # mask\n",
    "        masked_pos, masked_tokens, n_pred = mask_lm(input_ids, max_pred)\n",
    "        # padding\n",
    "        input_ids, segment_ids, masked_pos, masked_tokens = \\\n",
    "            zero_padding(input_ids, segment_ids, max_len, max_pred, n_pred, masked_pos, masked_tokens)\n",
    "        \n",
    "        if tokens_a_index + 1== tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_pos, masked_tokens, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_pos, masked_tokens, False])\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "confused-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_pos, masked_tokens, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], \\\n",
    "    self.masked_pos[idx], self.isNext[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "premium-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_sampler(batch_size, token_list, max_pred, max_len)\n",
    "input_ids, segment_ids, masked_pos, masked_tokens, isNext = zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intensive-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_pos, masked_tokens, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_pos),\\\n",
    "    torch.LongTensor(masked_tokens), torch.LongTensor(isNext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "latin-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(MyDataSet(input_ids, segment_ids, masked_pos, masked_tokens, isNext), batch_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-island",
   "metadata": {},
   "source": [
    "## 构建BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dried-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size\n",
    "batch_size = 6\n",
    "embedding_size = 768\n",
    "hidden_size = 768 * 4\n",
    "dim_k = dim_v = 64\n",
    "n_heads = 12\n",
    "n_layers = 6\n",
    "max_len = 30\n",
    "n_segments = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "robust-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "      Implementation of the gelu activation function.\n",
    "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "      Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "comfortable-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_length, n_segments):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.postion_embedding = nn.Embedding(max_length, embedding_size)\n",
    "        self.segment_embedding = nn.Embedding(n_segments, embedding_size)\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "    \n",
    "    def forward(self, X, seg):\n",
    "        seq_len = X.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(X)\n",
    "        embedding = self.token_embedding(X) + self.postion_embedding(pos) + self.segment_embedding(seg)\n",
    "        embedding = self.norm(embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "green-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, k_dim, v_dim, n_heads, hidden_size, n_layers, \\\n",
    "                 max_length, n_segments, \\\n",
    "                 dropout=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_size, max_length, n_segments)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(EncoderLayer(embedding_size, k_dim, v_dim, n_heads, hidden_size))\n",
    "        # task1\n",
    "        # shared weight with Token Embedding Layer\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc_mlm=nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "        embedding_weight = self.embedding.token_embedding.weight\n",
    "        self.fc_mlm.weight = embedding_weight\n",
    "        self.activate = gelu\n",
    "        # task2\n",
    "        self.fc_task_nps = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embedding_size, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        padding_mask = pad_mask(input_ids)\n",
    "        X = self.embedding(input_ids, segment_ids)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, padding_mask)\n",
    "            \n",
    "        # task1: MLM\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, X.size(2))\n",
    "        h_masked = torch.gather(X, 1, masked_pos)\n",
    "        h_masked = self.activate(self.linear(h_masked))\n",
    "        result_mlm = self.fc_mlm(h_masked)\n",
    "        # task2: predict isNext by first token(CLS)\n",
    "        reslult_isNext = self.fc_task_nps(X[:, 0])\n",
    "        return result_mlm, reslult_isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cellular-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "model = BERT(vocab_size, embedding_size, dim_k, dim_v, n_heads, hidden_size, n_layers, max_len, n_segments)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "concerned-authorization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss = 0.619410\n",
      "Epoch: 0020 loss = 0.594902\n",
      "Epoch: 0030 loss = 0.442325\n",
      "Epoch: 0040 loss = 0.274707\n",
      "Epoch: 0050 loss = 0.066766\n",
      "Epoch: 0060 loss = 0.005501\n",
      "Epoch: 0070 loss = 0.000794\n",
      "Epoch: 0080 loss = 0.000278\n",
      "Epoch: 0090 loss = 0.000183\n",
      "Epoch: 0100 loss = 0.000146\n",
      "Epoch: 0110 loss = 0.000122\n",
      "Epoch: 0120 loss = 0.000094\n",
      "Epoch: 0130 loss = 0.000082\n",
      "Epoch: 0140 loss = 0.000075\n",
      "Epoch: 0150 loss = 0.000049\n",
      "Epoch: 0160 loss = 0.000037\n",
      "Epoch: 0170 loss = 0.000026\n",
      "Epoch: 0180 loss = 0.000018\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(180):\n",
    "    output_mlm, output_isNext = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_mlm = loss(output_mlm.transpose(1, 2), masked_pos)\n",
    "    loss_mlm = (loss_mlm.float()).mean()\n",
    "    loss_isNext = loss(output_isNext, isNext)\n",
    "    l = loss_mlm + loss_isNext\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-mother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-taste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-fishing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-package",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-routine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-cargo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-aurora",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-issue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-toronto",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-novel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-inspiration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-coral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-graphic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-control",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-representation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-booth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-myrtle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-macintosh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-window",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-province",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-coupon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-peace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
