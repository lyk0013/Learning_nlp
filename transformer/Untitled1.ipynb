{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from random import randrange, shuffle, random, randint\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-witch",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    " text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # clean data\n",
    "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\n",
    "    word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
    "    word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "    for i, w in enumerate(word_list):\n",
    "        word2idx[w] = i + 4\n",
    "    idx2word = {i: w for i, w in enumerate(word2idx)}\n",
    "    vocab_size = len(word2idx)\n",
    "    return word2idx, idx2word, vocab_size, word_list, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "above-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2token(sentences):\n",
    "    token_list = list()\n",
    "    for sentence in sentences:\n",
    "        arr = [word2idx[s] for s in sentence.split()]\n",
    "        token_list.append(arr)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "balanced-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word, vocab_size, word_list, sentences = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brazilian-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = text2token(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-outreach",
   "metadata": {},
   "source": [
    "## BERT预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interesting-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_len = 30\n",
    "max_pred = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "iraqi-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "def zero_padding(input_ids, segment_ids, max_len, max_pred, n_pred, masked_pos, masked_tokens):\n",
    "    n_pad = max_len - len(input_ids)\n",
    "    input_ids.extend([0] * n_pad)\n",
    "    segment_ids.extend([0] * n_pad)\n",
    "    \n",
    "    # zero padding tokens\n",
    "    if max_pred > n_pred:\n",
    "        masked_tokens.extend([0] * n_pad)\n",
    "        masked_pos.extend([0] * n_pad)\n",
    "    return input_ids, segment_ids, masked_pos, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "labeled-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机MASK\n",
    "def mask_lm(input_ids, max_pred):\n",
    "    # 单句要预测的token个数(15%)\n",
    "    n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
    "    # 候选mask id 列表， 特殊标记不可mask\n",
    "    cand_maked_pos = [\n",
    "        i for i, token in enumerate(input_ids)\n",
    "        if token != word2idx['[CLS]'] and token != word2idx['[SEP]']\n",
    "    ]\n",
    "    shuffle(cand_maked_pos)\n",
    "    masked_tokens, masked_pos = [], []\n",
    "    for pos in cand_maked_pos[:n_pred]:\n",
    "        masked_pos.append(pos)\n",
    "        masked_tokens.append(input_ids[pos])\n",
    "        if random() < 0.8:  # 80% 替换成[MASK]\n",
    "            input_ids[pos] = word2idx['[MASK]']\n",
    "        elif random() > 0.9:  # 10% 替换成任意词\n",
    "            index = randint(0, vocab_size - 1)\n",
    "            while index < 4:  # 特殊标记不可替换\n",
    "                index = randint(0, vocab_size - 1)\n",
    "            input_ids[pos] = index\n",
    "    return masked_pos, masked_tokens, n_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "attractive-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取positive和negative样本, 比例为1:1\n",
    "def batch_sampler(batch_size, token_list, max_pred, max_len):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # randrange(stop):返回一个随机数\n",
    "        # 如果tokens_a_index + 1 = tokens_b_index, 则为positive，否则 negative\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        # mask\n",
    "        masked_pos, masked_tokens, n_pred = mask_lm(input_ids, max_pred)\n",
    "        # padding\n",
    "        input_ids, segment_ids, masked_pos, masked_tokens = \\\n",
    "            zero_padding(input_ids, segment_ids, max_len, max_pred, n_pred, masked_pos, masked_tokens)\n",
    "        \n",
    "        if tokens_a_index + 1== tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_pos, masked_tokens, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_pos, masked_tokens, False])\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "technological-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_sampler(batch_size, token_list, max_pred, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indirect-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "downtown-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 38,\n",
       " 39,\n",
       " 12,\n",
       " 17,\n",
       " 34,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 19,\n",
       " 18,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-scanner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-entry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-efficiency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-sandwich",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-vegetable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-controversy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-parade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 抽取positive和negative样本, 比例为1:1\n",
    "# 2. 随机MASK\n",
    "# 3. padding\n",
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        # MASK LM\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) \n",
    "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
    "        \n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:\n",
    "                input_ids[pos] = word2idx['[MASK]']\n",
    "            elif random() > 0.9:\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                while index < 4:\n",
    "                    index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index\n",
    "        # zero padding\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        # zero padding (100%-15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "            \n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-stress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-beach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-north",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-direction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-calgary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-chemistry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-wheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-purchase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-consultancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 抽取positive和negative样本, 比例为1:1\n",
    "# 2. 随机MASK\n",
    "# 3. padding\n",
    "def make_data():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        # MASK LM\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) \n",
    "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
    "        \n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:\n",
    "                input_ids[pos] = word2idx['[MASK]']\n",
    "            elif random() > 0.9:\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                while index < 4:\n",
    "                    index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = index\n",
    "        # zero padding\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        # zero padding (100%-15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "            \n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids, segment_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pos, masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-mystery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
