{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cutting-witch",
   "metadata": {},
   "source": [
    "## Attention机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-likelihood",
   "metadata": {},
   "source": [
    "**Attention机制：**Decoder层会将输入的Encoder隐藏层进行softmax计算，然后加权求和，与Decoder输入拼接。这样做的好处是可以将Decoder的每一个时间步都利用与该时间步最相关的Encoder编码信息，这样可以更好地学习到两种语言中对应位置单词的关系。\n",
    "\n",
    "![seq2seq](./images/seq2seq.svg)\n",
    "<div align=center>不含Attention机制，Encoder输出隐藏层最后一层向量</div>  \n",
    "\n",
    "\n",
    "![seq2seq——attention](./images/seq2seq_attention.png)\n",
    "<div align=center>Attention机制，Encoder输出隐藏层所有时间步向量</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-plasma",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-sweet",
   "metadata": {},
   "source": [
    "要点：\n",
    "- Self Attention\n",
    "- Multi-Head Attention\n",
    "- Positional Embedding\n",
    "- Add & Layer Normalization\n",
    "- Encoder and Decoder\n",
    "\n",
    "相比于seq2seq-Attention，transformer的优势：\n",
    "* self Attention 取代LSTM，可以实现并发；而且同时处理上下文信息\n",
    "* 增加了Q,K,V 参数矩阵，使模型具有更强大的表现力\n",
    "* Multi-Head Attention，并发且可以采集到不同特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-dublin",
   "metadata": {},
   "source": [
    "![illustrated transformer](./images/transformer-illustrated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-perception",
   "metadata": {},
   "source": [
    "### Encoder 数据流"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "academic-letters",
   "metadata": {},
   "source": [
    "![encoder](./images/encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-publisher",
   "metadata": {},
   "source": [
    "### Decoder数据流"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sexual-blank",
   "metadata": {},
   "source": [
    "![decoder](./images/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-platinum",
   "metadata": {},
   "source": [
    "### Transformer代码实现--手动编写Layer\n",
    "* 生成测试数据\n",
    "* 初始化超参数\n",
    "* 初始化模型\n",
    "* 模型训练\n",
    "* 预测-BeamSearch  \n",
    "[代码链接](./Task2-Pytorch_Transformer_with_Custom_Layer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-luxury",
   "metadata": {},
   "source": [
    "## Transformer问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-topic",
   "metadata": {},
   "source": [
    "### Self-Attention相比于seq2seq的Attention有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-listing",
   "metadata": {},
   "source": [
    "* 1. Self-Attention 可以并行化，不存在时序的限制（RNN）\n",
    "* 2. Self-Attention的Encoder可以同时关注到上下文信息，而attention只能看到上文信息\n",
    "* 3. Self—Attention在计算过程中会直接将句子中任意两个单词直接联系起来，更容易捕获句子中长距离的依赖特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-commons",
   "metadata": {},
   "source": [
    "### Transformer中的softmax计算为什么需要除以$\\sqrt{d_k}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-microphone",
   "metadata": {},
   "source": [
    "随着k_dim的增长，点积模型的值通常有比较大的方差，即存在部分值很大/很小的极端情况；而softmax对于值之间的相对大小有很强的放大作用。所以为了避免进入softmax饱和区，要对点积模型进行缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-manchester",
   "metadata": {},
   "source": [
    "### Transformer中attention score计算时候如何mask掉padding位置?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-cornwall",
   "metadata": {},
   "source": [
    "会生成一个与attention score大小一致的pad_mask矩阵（0/1）  \n",
    "softmax运算前，会将attention score与pad_mask值为零对应的位置赋予一个极小负数，这样，在softmax运算时，该值会趋近于零。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-forwarding",
   "metadata": {},
   "source": [
    "### 为什么Transformer中加入了positional embedding？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-incidence",
   "metadata": {},
   "source": [
    "由于 Transformer 模型没有循环神经网络的迭代操作，所以我们必须提供每个字的位置信息给 Transformer，这样它才能识别出语言中的顺序关系.\n",
    "* 位置编码应该为每个字输出唯一的编码\n",
    "* 不同长度的句子之间，任何两个字之间的差值应该保持一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-cleaners",
   "metadata": {},
   "source": [
    "### Encoder和Decoder中参数有哪些是一致的？哪些可能不一致？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-gabriel",
   "metadata": {},
   "source": [
    "一致的参数：\n",
    "* Batch_size\n",
    "* Embedding_size\n",
    "* k_dim=q_dim\n",
    "* n_heads\n",
    "* q_dim\n",
    "\n",
    "\n",
    "不一致的参数：\n",
    "* max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-comedy",
   "metadata": {},
   "source": [
    "### Multi-Head Attention 在计算时还是按一个大矩阵计算，跟不是多头有什么区别？优势在哪里？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "expensive-victory",
   "metadata": {},
   "source": [
    "由于Embedding会按n_heads进行均匀划分，在计算的时候仍然是一个大矩阵（所有头）进行计算，但是在计算Attention的时候是按照小区间进行计算的，\n",
    "![Multi-Head Attn from SVD](./images/task2_qa_3.6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-bathroom",
   "metadata": {},
   "source": [
    "### Layer Normalization 相比 Batch Normalization的优点是？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "governmental-combination",
   "metadata": {},
   "source": [
    "![LN & BN](./images/task2_qa_3.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-minister",
   "metadata": {},
   "source": [
    "BN、LN可以看作横向和纵向的区别。\n",
    "经过归一化再输入激活函数，得到的值大部分会落入非线性函数的线性区，导数远离导数饱和区，避免了梯度消失，这样来加速训练收敛过程。\n",
    "\n",
    "LayerNorm这类归一化技术，目的就是让每一层的分布稳定下来，让后面的层可以在前面层的基础上安心学习知识。\n",
    "\n",
    "BatchNorm就是通过对batch size这个维度归一化来让分布稳定下来。LayerNorm则是通过对Hidden size这个维度归一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-covering",
   "metadata": {},
   "source": [
    "### 残差连接的优势是什么？是为了解决什么问题？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dependent-fortune",
   "metadata": {},
   "source": [
    "![residual1](./images/task2_qa_3.8_1.png)\n",
    "![residual2](./images/task2_qa_3.8_2.png)\n",
    "![residual3](./images/task2_qa_3.8_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-success",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
