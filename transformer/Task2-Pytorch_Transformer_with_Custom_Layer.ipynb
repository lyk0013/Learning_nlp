{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import pad_mask, subsequence_mask\n",
    "from utils import PositionalEncoding, EncoderLayer, DecoderLayer\n",
    "from utils import TransformerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "crucial-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData:\n",
    "    def __init__(self):\n",
    "        self.src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
    "        self.src_vocab_size = len(self.src_vocab)\n",
    "        self.src_idx2word = {i: w for i, w in enumerate(self.src_vocab)}\n",
    "\n",
    "        self.tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "        self.tgt_idx2word = {i: w for i, w in enumerate(self.tgt_vocab)}\n",
    "        self.tgt_vocab_size = len(self.tgt_vocab)\n",
    "        \n",
    "    def make_data(self, sentences):\n",
    "        enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "        for i in range(len(sentences)):\n",
    "            enc_input = [[self.src_vocab[n] for n in sentences[i][0].split()]]\n",
    "            dec_input = [[self.tgt_vocab[n] for n in sentences[i][1].split()]]        \n",
    "            dec_output = [[self.tgt_vocab[n] for n in sentences[i][2].split()]]\n",
    "\n",
    "            enc_inputs.extend(enc_input)\n",
    "            dec_inputs.extend(dec_input)\n",
    "            dec_outputs.extend(dec_output)\n",
    "\n",
    "        return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "    \n",
    "    def get_test_data(self, sentences):\n",
    "        return self.make_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lightweight-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positonal_encoder = PositionalEncoding(embedding_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = EncoderLayer(embedding_size, k_dim, v_dim, n_heads, hidden_size)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        mask = pad_mask(X)\n",
    "        X = self.embeddding(X)\n",
    "        X = self.positonal_encoder(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proved-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positonal_encoder = PositionalEncoding(embedding_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = DecoderLayer(embedding_size, k_dim, v_dim, n_heads, hidden_size)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def forward(self, X, X_encoder, encoder_inputs):\n",
    "        self_attention_mask = pad_mask(X)\n",
    "        subseq_mask = subsequence_mask(X)\n",
    "        # torch.gt(matrix, target), if element in matrix, if element > target, return 1, else 0\n",
    "        self_attention_mask = torch.gt((self_attention_mask + subseq_mask), 0).to(X.device)\n",
    "        decoder_encoder_attention_mask = pad_mask(encoder_inputs, X)\n",
    "        \n",
    "        X = self.embeddding(X)\n",
    "        X = self.positonal_encoder(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, X_encoder, self_attention_mask, decoder_encoder_attention_mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "overhead-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, \n",
    "                 embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(encoder_vocab_size, embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(decoder_vocab_size, embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(embedding_size, decoder_vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, X_encoder, X_decoder):\n",
    "        Y_encoder = self.encoder(X_encoder)\n",
    "        Y_decoder = self.decoder(X_decoder, Y_encoder, X_encoder)\n",
    "        # Y: [batch_size, length_decoder, vocab_decoder_size]\n",
    "        Y = self.linear(Y_decoder)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-virtue",
   "metadata": {},
   "source": [
    "### 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "artificial-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "        # enc_input           dec_input         dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "custom_data = CustomData()\n",
    "enc_inputs, dec_inputs, dec_outputs = custom_data.get_test_data(sentences)\n",
    "dataset = TransformerDataset(enc_inputs, dec_inputs, dec_outputs)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "minus-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    enc_inputs, dec_inputs, dec_outputs = [x for x in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "conscious-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 5, 0],\n",
       "        [1, 2, 3, 4, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-highlight",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handy-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "lr = 1e-4\n",
    "weight_decay =1e-5\n",
    "epochs = 5\n",
    "num_layers = 6\n",
    "n_heads = 8\n",
    "k_dim = 64\n",
    "v_dim = 64\n",
    "embedding_size = 512\n",
    "hidden_size = 300\n",
    "encoder_vocab_size = custom_data.src_vocab_size\n",
    "decoder_vocab_size = custom_data.tgt_vocab_size\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-grill",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hired-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    encoder_vocab_size, decoder_vocab_size,\n",
    "    embedding_size, k_dim, v_dim, n_heads, hidden_size, num_layers\n",
    ")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr, \n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-entity",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "round-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 0.468823\n",
      "Epoch: 0002 loss = 0.229420\n",
      "Epoch: 0003 loss = 0.162966\n",
      "Epoch: 0004 loss = 0.085150\n",
      "Epoch: 0005 loss = 0.063321\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in data_loader:\n",
    "        enc_inputs, dec_inputs, dec_outputs = [x.to(device) for x in batch]\n",
    "        # outputs: [batch_size, length, vocab_size]\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        l = loss(outputs.transpose(2, 1), dec_outputs)\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(l))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-confirmation",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "seven-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch:\n",
    "    def __init__(self, model, k=2, start_symbol=None, stop_symbol=None, max_predict_length=1000):\n",
    "        self.model = model\n",
    "        self.k, self.max_predict_length = k, max_predict_length\n",
    "        self.start_symbol, self.stop_symbol= start_symbol, stop_symbol\n",
    "        \n",
    "    def greedy_decoder(self, X):\n",
    "        X_encoder = X.view(1, -1)\n",
    "        Y_encoder = model.encoder(X_encoder)\n",
    "        \n",
    "        next_word = self.start_symbol\n",
    "        dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "        while True:\n",
    "            # concate next word\n",
    "            dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_word]],dtype=enc_input.dtype)],-1)\n",
    "            # run decoder and linear\n",
    "            Y_decoder = model.decoder(dec_input, Y_encoder, X_encoder)\n",
    "            word_probability = model.linear(Y_decoder)\n",
    "            word_probability = word_probability.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "            next_word = word_probability[-1]\n",
    "            \n",
    "            if next_word in self.stop_symbol or dec_input.size(1)>=self.max_predict_length:\n",
    "                dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_word]],dtype=enc_input.dtype)],-1)\n",
    "                return dec_input.squeeze(0)\n",
    "    \n",
    "    def search(self, X):\n",
    "        K = self.k\n",
    "        X_encoder = X.view(1, -1)\n",
    "        Y_encoder = model.encoder(X_encoder)\n",
    "        \n",
    "        next_word = self.start_symbol\n",
    "        dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_word]],dtype=enc_input.dtype)],-1)\n",
    "        sequences = [(dec_input, 1)]\n",
    "        for _ in range(self.max_predict_length):\n",
    "            # concate next word\n",
    "            counter = 0\n",
    "            all_condidates = []\n",
    "            for (sequence, prob) in sequences:\n",
    "                dec_input = sequence\n",
    "                # run decoder and linear\n",
    "                Y_decoder = model.decoder(dec_input, Y_encoder, X_encoder)\n",
    "                word_probability = model.linear(Y_decoder)\n",
    "                word_probability = nn.Softmax(dim=-1)(word_probability.squeeze(0))\n",
    "                word_probability = word_probability[-1].squeeze(0)\n",
    "                for i in range(word_probability.size(-1)):\n",
    "                    candidate = (\n",
    "                        torch.cat([dec_input.detach(),torch.tensor([[i]],dtype=enc_input.dtype)],-1),\n",
    "                        prob * word_probability[i].item()\n",
    "                    )\n",
    "                    all_condidates.append(candidate)\n",
    "            ordered = sorted(all_condidates, key=lambda x:x[1], reverse=True)\n",
    "            sequences = ordered[:K]\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "attempted-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol= custom_data.tgt_vocab['S']\n",
    "stop_symbol = [custom_data.tgt_vocab['.'], custom_data.tgt_vocab['E']]\n",
    "stop_symbol = [custom_data.tgt_vocab['E']]\n",
    "max_predict_length = 6\n",
    "enc_inputs, _, _ = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "round-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 4, 0],\n",
       "         [1, 2, 3, 5, 0]]),\n",
       " 2,\n",
       " 6)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs, len(enc_inputs), start_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "regional-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search = BeamSearch(model, k=5, start_symbol=start_symbol, stop_symbol=stop_symbol, \n",
    "                         max_predict_length=max_predict_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "behavioral-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 1, 2, 3, 4, 8, 7])\n",
      "['ich', 'mochte', 'ein', 'bier', 'P']\n",
      "['S', 'i', 'want', 'a', 'beer', '.', 'E']\n",
      "tensor([6, 1, 2, 3, 5, 8, 7])\n",
      "['ich', 'mochte', 'ein', 'cola', 'P']\n",
      "['S', 'i', 'want', 'a', 'coke', '.', 'E']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(enc_inputs)):\n",
    "    predict = beam_search.greedy_decoder(enc_inputs[i])\n",
    "    print(predict)\n",
    "    print([custom_data.src_idx2word[x.item()] for x in enc_inputs[i]])\n",
    "    print([custom_data.tgt_idx2word[x.item()] for x in predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "frequent-parcel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[6, 1, 2, 3, 4, 8, 7]]), 0.7471778901401462), (tensor([[6, 1, 2, 3, 4, 4, 8]]), 0.023645795037044923), (tensor([[6, 1, 2, 2, 3, 4, 8]]), 0.020296269745991666), (tensor([[6, 1, 2, 3, 4, 8, 4]]), 0.014324764169617329), (tensor([[6, 1, 2, 4, 8, 7, 7]]), 0.011409323775865048)]\n",
      "['ich', 'mochte', 'ein', 'bier', 'P']\n",
      "[['S', 'i', 'want', 'a', 'beer', '.', 'E'], ['S', 'i', 'want', 'a', 'beer', 'beer', '.'], ['S', 'i', 'want', 'want', 'a', 'beer', '.'], ['S', 'i', 'want', 'a', 'beer', '.', 'beer'], ['S', 'i', 'want', 'beer', '.', 'E', 'E']]\n",
      "[(tensor([[6, 1, 2, 3, 5, 8, 7]]), 0.729272290460904), (tensor([[6, 1, 2, 3, 4, 8, 7]]), 0.021989770150991506), (tensor([[6, 1, 2, 2, 3, 5, 8]]), 0.020796378916303152), (tensor([[6, 1, 2, 3, 5, 5, 8]]), 0.017689265194172424), (tensor([[6, 1, 2, 5, 8, 7, 7]]), 0.017455388161037484)]\n",
      "['ich', 'mochte', 'ein', 'cola', 'P']\n",
      "[['S', 'i', 'want', 'a', 'coke', '.', 'E'], ['S', 'i', 'want', 'a', 'beer', '.', 'E'], ['S', 'i', 'want', 'want', 'a', 'coke', '.'], ['S', 'i', 'want', 'a', 'coke', 'coke', '.'], ['S', 'i', 'want', 'coke', '.', 'E', 'E']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(enc_inputs)):\n",
    "    predict_sequences = beam_search.search(enc_inputs[i])\n",
    "    print(predict_sequences)\n",
    "    print([custom_data.src_idx2word[x.item()] for x in enc_inputs[i]])\n",
    "    print([[custom_data.tgt_idx2word[x.item()] for x in seq.squeeze(0)] for (seq, _) in predict_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "several-hayes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[6, 1, 2, 3, 5, 8, 7]]), 0.729272290460904),\n",
       " (tensor([[6, 1, 2, 3, 4, 8, 7]]), 0.021989770150991506),\n",
       " (tensor([[6, 1, 2, 2, 3, 5, 8]]), 0.020796378916303152),\n",
       " (tensor([[6, 1, 2, 3, 5, 5, 8]]), 0.017689265194172424),\n",
       " (tensor([[6, 1, 2, 5, 8, 7, 7]]), 0.017455388161037484)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "classical-madness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0633, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-fortune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-breath",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-module",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-utility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-negotiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-mounting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
