{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alleged-newcastle",
   "metadata": {},
   "source": [
    "![](./images/transformer_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-video",
   "metadata": {},
   "source": [
    "## 学习概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-parameter",
   "metadata": {},
   "source": [
    "![transformer学习概览](./images/transformer_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-teach",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "\n",
    "静态词向量\n",
    "* Language Model\n",
    "* word2vec\n",
    "* GloVe  \n",
    "\n",
    "\n",
    "动态词向量\n",
    "* ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-special",
   "metadata": {},
   "source": [
    "**静态词向量和动态词向量的区别**  \n",
    "**静态词向量：**  在模型训练后，一个词的向量表示是唯一且固定的，上下文无关的。\n",
    "- 优点： 相比于n-gram，是低维且稠密向量；可以满足相似和类比等评价指标\n",
    "- 缺点： 无法表达一词多义\n",
    "\n",
    "**动态词向量：**  一个词的向量是随上下文动态变化的。\n",
    "- 优点： 一词多义，低维且稠密\n",
    "- 缺点： 需要大量数据进行训练\n",
    "\n",
    "**什么是预训练？**\n",
    "**预训练：**通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。无论是Wordvec还是BERT都是预训练模型，主要学习的就是一个词或者一个文本序列的向量/矩阵表示。\n",
    "\n",
    "**什么是Fine-Tune？**\n",
    "**Fine-Tune/微调/精调：**在预训练模型的基础上，针对具体的任务微调网络模型。不只是BERT， GloVe、word2vec也可以根据任务微调模型参数，也可以认为是迁移模型的一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-width",
   "metadata": {},
   "source": [
    "### Language Model - 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-tyler",
   "metadata": {},
   "source": [
    "语言模型的基本任务：给定一段文本，语言模型根据历史上下文对下一时刻的词进行预测。即$P(w_t|w_1w_2...w_{t-1})$,为避免依赖过长，会利用**马尔科夫假设**，即$P(w_t|w_{1:t-1}) = P(w_t|w_{t-n+1:t-1})$\n",
    "\n",
    "训练结束后，**Embedding层**学得的向量就是词向量。\n",
    "\n",
    "由于序列较长，训练中会出现梯度消失/梯度爆炸，一般使用**梯度裁剪**来解决\n",
    "\n",
    "**优点：** 可以生成低维词向量  \n",
    "**缺点：** 只利用了上文，缺少了下文的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-convertible",
   "metadata": {},
   "source": [
    "![LM](./images/language_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-appreciation",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-abortion",
   "metadata": {},
   "source": [
    "上文的Language Model中，词向量只是一个中间产物；而本节的word2vec（2013年）是设计出来专门训练词向量的算法。  \n",
    "**假设：**一个词的含义，可由它周围的词来表示  \n",
    "相比于LM：\n",
    "* Embeddings 直接sum, 而不是拼接\n",
    "* 舍弃隐藏层\n",
    "* 采用 hierachical softmax 和 negative sampling \n",
    "\n",
    "**输入层和输出层都有词向量矩阵，可以只取输入或拼接两者**\n",
    "\n",
    "word2vec 有两种训练方式：\n",
    "* CBOW: 周围词来预测中心词\n",
    "* skip-gram: 中心词来预测周围词\n",
    "\n",
    "![word2vec](./images/word2vec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-mainstream",
   "metadata": {},
   "source": [
    "### GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-summit",
   "metadata": {},
   "source": [
    "GloVe是基于全局语料库的，主要是利用了全局共现矩阵。  \n",
    "GloVe实际上是有监督学习，label是log（贡献次数），是一个回归模型。  \n",
    "loss函数是加权回归损失函数（最小平方损失）  \n",
    "![GloVe](./images/GloVe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-breed",
   "metadata": {},
   "source": [
    "### ELMo - Embeddings from Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-campaign",
   "metadata": {},
   "source": [
    "要点：\n",
    "* 1. 字符级别的输入\n",
    "* 2. 基于字符卷积的词表示层：CNN，Highway\n",
    "* 3. 双向LSTM\n",
    "\n",
    "ELMo可以输出三层词向量， 每层词向量适合的任务都不同：\n",
    "* 底层更侧重语法和句法， 顶层更侧重语义\n",
    "* 第一层更适合词性标注任务\n",
    "* 第二层更适合词义消歧任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-algebra",
   "metadata": {},
   "source": [
    "![ELMo](./images/ELMo.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-light",
   "metadata": {},
   "source": [
    "## Transformer简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-patrol",
   "metadata": {},
   "source": [
    "要点：\n",
    "- self Attention\n",
    "- Multi-Head Attention\n",
    "- Positional Embedding\n",
    "- Add & Layer Normalization\n",
    "- Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-commitment",
   "metadata": {},
   "source": [
    "![Transformer](./images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-costa",
   "metadata": {},
   "source": [
    "## 预训练模型简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-biotechnology",
   "metadata": {},
   "source": [
    "预训练模型通常指代的是预训练语言模型，目前主要指的是BERT和GPT等基于深层Transformer的深度模型。  \n",
    "预训练模型的主要特点：\n",
    "* 大数据：希望从大规模语料库中学习得到预料的共性知识或基础知识\n",
    "* 大模型：大模型才能容纳大数据，Multi-Head Attention 可以并行，且可有效捕获不同词之间的关联程度\n",
    "* 大算力：CPU擅长串行运算和逻辑，GPU擅长大规模并行运算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-definition",
   "metadata": {},
   "source": [
    "## NLP任务体系简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-breakfast",
   "metadata": {},
   "source": [
    "NLP的子任务较多，归纳后可以分为三类基础任务和四类应用任务。\n",
    "\n",
    "**基础任务**\n",
    "* 文本分类：分类、匹配、蕴含等分类任务\n",
    "* 结构预测：序列标注、分割、图结构生成等\n",
    "* 序列到序列：机器翻译，聊天机器人，摘要生成等\n",
    "\n",
    "**应用任务**：\n",
    "* 信息抽取\n",
    "* 情感分析\n",
    "* 问答系统\n",
    "* 机器翻译\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.242px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
