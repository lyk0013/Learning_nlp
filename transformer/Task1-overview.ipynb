{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-homework",
   "metadata": {},
   "source": [
    "![](./images/transformer_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-algebra",
   "metadata": {},
   "source": [
    "## 学习概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-lingerie",
   "metadata": {},
   "source": [
    "![transformer学习概览](./images/transformer_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-huntington",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "\n",
    "静态词向量\n",
    "* Language Model\n",
    "* word2vec\n",
    "* GloVe  \n",
    "\n",
    "\n",
    "动态词向量\n",
    "* ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-burner",
   "metadata": {},
   "source": [
    "**静态词向量和动态词向量的区别**  \n",
    "**静态词向量：**  在模型训练后，一个词的向量表示是唯一且固定的，上下文无关的。\n",
    "- 优点： 相比于n-gram，是低维且稠密向量；可以满足相似和类比等评价指标\n",
    "- 缺点： 无法表达一词多义\n",
    "\n",
    "**动态词向量：**  一个词的向量是随上下文动态变化的。\n",
    "- 优点： 一词多义，低维且稠密\n",
    "- 缺点： 需要大量数据进行训练\n",
    "\n",
    "**什么是预训练？**\n",
    "**预训练：**通过自监督学习从大规模数据中获得与具体任务无关的预训练模型。无论是Wordvec还是BERT都是预训练模型，主要学习的就是一个词或者一个文本序列的向量/矩阵表示。\n",
    "\n",
    "**什么是Fine-Tune？**\n",
    "**Fine-Tune/微调/精调：**在预训练模型的基础上，针对具体的任务微调网络模型。不只是BERT， GloVe、word2vec也可以根据任务微调模型参数，也可以认为是迁移模型的一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-ethernet",
   "metadata": {},
   "source": [
    "### Language Model - 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-pillow",
   "metadata": {},
   "source": [
    "语言模型的基本任务：给定一段文本，语言模型根据历史上下文对下一时刻的词进行预测。即$P(w_t|w_1w_2...w_{t-1})$,为避免依赖过长，会利用**马尔科夫假设**，即$P(w_t|w_{1:t-1}) = P(w_t|w_{t-n+1:t-1})$\n",
    "\n",
    "训练结束后，**Embedding层**学得的向量就是词向量。\n",
    "\n",
    "由于序列较长，训练中会出现梯度消失/梯度爆炸，一般使用**梯度裁剪**来解决\n",
    "\n",
    "**优点：** 可以生成低维词向量  \n",
    "**缺点：** 只利用了上文，缺少了下文的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-estonia",
   "metadata": {},
   "source": [
    "![LM](./images/language_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-kennedy",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-checklist",
   "metadata": {},
   "source": [
    "上文的Language Model中，词向量只是一个中间产物；而本节的word2vec（2013年）是设计出来专门训练词向量的算法。  \n",
    "**假设：**一个词的含义，可由它周围的词来表示  \n",
    "相比于LM：\n",
    "* Embeddings 直接sum, 而不是拼接\n",
    "* 舍弃隐藏层\n",
    "* 采用 hierachical softmax 和 negative sampling \n",
    "\n",
    "**输入层和输出层都有词向量矩阵，可以只取输入或拼接两者**\n",
    "\n",
    "word2vec 有两种训练方式：\n",
    "* CBOW: 周围词来预测中心词\n",
    "* skip-gram: 中心词来预测周围词\n",
    "\n",
    "![word2vec](./images/word2vec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-verification",
   "metadata": {},
   "source": [
    "### GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-infrared",
   "metadata": {},
   "source": [
    "GloVe是基于全局语料库的，主要是利用了全局共现矩阵。  \n",
    "GloVe实际上是有监督学习，label是log（贡献次数），是一个回归模型。  \n",
    "loss函数是加权回归损失函数（最小平方损失）  \n",
    "![GloVe](./images/glove.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-proxy",
   "metadata": {},
   "source": [
    "### ELMo - Embeddings from Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-dutch",
   "metadata": {},
   "source": [
    "要点：\n",
    "* 1. 字符级别的输入\n",
    "* 2. 基于字符卷积的词表示层：CNN，Highway\n",
    "* 3. 双向LSTM\n",
    "\n",
    "ELMo可以输出三层词向量， 每层词向量适合的任务都不同：\n",
    "* 底层更侧重语法和句法， 顶层更侧重语义\n",
    "* 第一层更适合词性标注任务\n",
    "* 第二层更适合词义消歧任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-characterization",
   "metadata": {},
   "source": [
    "![ELMo](./images/ELMo.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-calvin",
   "metadata": {},
   "source": [
    "## Transformer简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-curtis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "amazing-college",
   "metadata": {},
   "source": [
    "## 预训练模型简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-worker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "generic-triple",
   "metadata": {},
   "source": [
    "## NLP任务体系简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-guard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.242px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
