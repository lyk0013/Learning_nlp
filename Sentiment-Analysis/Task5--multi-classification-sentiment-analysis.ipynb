{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchtext.datasets import SogouNews, AG_NEWS\n",
    "import torchtext.vocab as Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import get_vocab, get_tokenized\n",
    "from utils import evaluate, epoch_time\n",
    "from utils import train as trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载AG_NEWS数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = AG_NEWS(\n",
    "    root='./datasets', \n",
    "    split=('train','test')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 7600)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = list(train), list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 24000, 7600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"Guantanamo Prisoner Goes Before Tribunal (AP) AP - A U.S. military panel heard the case Wednesday of a Guantanamo Bay prisoner accused of fighting for Afghanistan's ousted Taliban regime, as a U.S. judge ordered the government to release records of alleged prisoner abuse at the American base.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, vocab, max_l=500):\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l-len(x))\n",
    "    tokenized_data = get_tokenized(data)\n",
    "    features = torch.LongTensor(\n",
    "        [pad(vocab.lookup_indices(words)) for words in tokenized_data]\n",
    "    )\n",
    "    labels = torch.tensor(\n",
    "        [score-1 for (score, _) in data],\n",
    "        dtype = torch.int64\n",
    "    )\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NewsDataset(*preprocess(train, vocab, max_length))\n",
    "valid_set = NewsDataset(*preprocess(valid, vocab, max_length))\n",
    "test_set = NewsDataset(*preprocess(test, vocab, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = dict(Counter([label.item() for label in train_set.labels]))\n",
    "weights = 1./torch.tensor(\n",
    "    [counter[i] for i in range(4)], \n",
    "    dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_weights = weights[train_set.labels]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weights, \n",
    "    num_samples = len(samples_weights),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler\n",
    ")\n",
    "valid_iter = DataLoader(\n",
    "    valid_set, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./datasets/glove\"\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count = 0\n",
    "    for i, word in  enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print('There are %d oov words.' % oov_count)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16180 oov words.\n"
     ]
    }
   ],
   "source": [
    "glove_100 = load_pretrained_embedding(vocab.get_itos(), glove_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设计模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, V, E, kernels, channels, O, weights, dropout=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(V, E)\n",
    "        self.constant_embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=weights,\n",
    "            freeze=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.convs = nn.ModuleList()\n",
    "        for kernel_size, channel_size in zip(kernels, channels):\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=2*E,\n",
    "                    out_channels=channel_size,\n",
    "                    kernel_size=kernel_size\n",
    "                )\n",
    "            )\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(sum(channels), O)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.cat(\n",
    "            (self.embedding(X), self.constant_embedding(X)),\n",
    "            dim=2\n",
    "        )\n",
    "        X = X.permute(0, 2, 1)\n",
    "        X = torch.cat(\n",
    "            [self.pool(torch.relu(conv(X))).squeeze(-1) for conv in self.convs],\n",
    "            dim=1\n",
    "        )\n",
    "        X = self.fc(self.dropout(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_length = len(vocab)\n",
    "Embedding_dim = 100\n",
    "Output_dim = 4\n",
    "lr = 1e-3\n",
    "Epochs = 5\n",
    "dropout = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "model = TextCNN(Vocab_length, embed_size, kernel_sizes, nums_channels,\n",
    "                Output_dim, glove_100, dropout\n",
    "               )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.278 | Train Acc: 90.55%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 90.48%\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.218 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 90.61%\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.185 | Train Acc: 93.98%\n",
      "\t Val. Loss: 0.318 |  Val. Acc: 90.76%\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.151 | Train Acc: 95.06%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 90.85%\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.131 | Train Acc: 95.75%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 91.24%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = trainer(\n",
    "        model, \n",
    "        train_iter, \n",
    "        optimizer, \n",
    "        loss,\n",
    "        device\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, loss,device)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/rnn-best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
