{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torchtext.vocab as Vocab\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import get_vocab_imdb, get_tokenized_imdb, get_tokenizer\n",
    "from utils import evaluate, epoch_time\n",
    "from utils import train as trainer\n",
    "from utils import preprocess_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = IMDB(split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = list(train), list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab = get_vocab_imdb(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data.TensorDataset(*preprocess_imdb(train, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./datasets/glove\"\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count = 0\n",
    "    for i, word in  enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print('There are %d oov words.' % oov_count)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14719 oov words.\n"
     ]
    }
   ],
   "source": [
    "glove_100 = load_pretrained_embedding(vocab.get_itos(), glove_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # 不参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.convs = nn.ModuleList()\n",
    "        # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(in_channels=2*embed_size, out_channels=c,kernel_size=k)\n",
    "            )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # 将两个嵌入层的输出按词向量连接（N, L, C）\n",
    "        embeddings = torch.cat((self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "        # con1d的输入格式：(N, C_in, L)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # 全局池化，展开，然后连接\n",
    "#         encoding = torch.cat(\n",
    "#             [self.pool(conv(embeddings)).squeeze(-1) for conv in self.convs],  # 未加 F.relu\n",
    "#             dim=1\n",
    "#         )\n",
    "        encoding = torch.cat(\n",
    "            [self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # 应用丢弃法后，使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_length = len(vocab)\n",
    "Embedding_dim = 100\n",
    "Output_dim = 2\n",
    "lr = 1e-3\n",
    "Epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14719 oov words.\n"
     ]
    }
   ],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "model = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)\n",
    "model.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.get_itos(), glove_vocab)\n",
    ")\n",
    "model.embedding.weight.requires_grad = False  # 该嵌入层不需要训练\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.556 | Train Acc: 71.25%\n",
      "\t Val. Loss: 0.418 |  Val. Acc: 82.32%\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.346 | Train Acc: 86.07%\n",
      "\t Val. Loss: 0.338 |  Val. Acc: 86.28%\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.256 | Train Acc: 90.15%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 88.03%\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.193 | Train Acc: 92.95%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 88.35%\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.150 | Train Acc: 94.64%\n",
      "\t Val. Loss: 0.308 |  Val. Acc: 88.35%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = trainer(\n",
    "        model, \n",
    "        train_iter, \n",
    "        optimizer, \n",
    "        loss,\n",
    "        device\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, loss,device)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/rnn-best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
